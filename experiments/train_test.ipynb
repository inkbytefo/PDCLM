{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDCLMBase Model Training Test\n",
    "\n",
    "Bu notebook PDCLMBase modelinin pretraining iÅŸlemini test eder.\n",
    "\n",
    "Hedefler:\n",
    "1. Model import ve initialization\n",
    "2. WikiText verisi ile 100 iterasyon training\n",
    "3. Loss tracking ve visualization\n",
    "4. Final loss < 1.0 hedefi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our model and utilities\n",
    "from src.model import PDCLMBase, pretrain_step, create_batches\n",
    "from src.utils import load_wikitext_data, visualize_training_curve\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load WikiText data\n",
    "data_path = \"../data/raw/wikitext_sample.txt\"\n",
    "print(f\"ðŸ“– Loading data from: {data_path}\")\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    raw_text = load_wikitext_data(data_path)\n",
    "    print(f\"âœ… Data loaded successfully\")\n",
    "    print(f\"ðŸ“ Text length: {len(raw_text):,} characters\")\n",
    "    print(f\"ðŸ“ Sample: {raw_text[:100]}...\")\n",
    "else:\n",
    "    print(f\"âŒ Data file not found: {data_path}\")\n",
    "    # Create sample data for testing\n",
    "    raw_text = \"This is a sample text for testing the PDCLM model. \" * 1000\n",
    "    print(f\"ðŸ“ Using synthetic data: {len(raw_text):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "print(\"ðŸ¤– Initializing PDCLMBase model...\")\n",
    "\n",
    "# Model configuration\n",
    "embed_dim = 256\n",
    "num_layers = 4\n",
    "heads = 4\n",
    "window_size = 512\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - embed_dim: {embed_dim}\")\n",
    "print(f\"  - num_layers: {num_layers}\")\n",
    "print(f\"  - heads: {heads}\")\n",
    "print(f\"  - window_size: {window_size}\")\n",
    "\n",
    "# Create model\n",
    "model = PDCLMBase(\n",
    "    embed_dim=embed_dim,\n",
    "    num_layers=num_layers,\n",
    "    heads=heads,\n",
    "    window_size=window_size\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ… Model created and moved to {device}\")\n",
    "print(f\"ðŸ“Š Model parameters: {model.count_parameters():,}\")\n",
    "print(f\"ðŸ“‹ Model info: {model.get_model_info()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"ðŸ”§ Optimizer configured:\")\n",
    "print(f\"  - Learning rate: {learning_rate}\")\n",
    "print(f\"  - Optimizer: AdamW\")\n",
    "print(f\"  - Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 10000  # characters per batch\n",
    "num_iterations = 100\n",
    "log_interval = 20\n",
    "\n",
    "print(f\"ðŸŽ¯ Training configuration:\")\n",
    "print(f\"  - Batch size: {batch_size:,} characters\")\n",
    "print(f\"  - Iterations: {num_iterations}\")\n",
    "print(f\"  - Log interval: {log_interval}\")\n",
    "\n",
    "# Create batches from text\n",
    "print(f\"ðŸ“¦ Creating batches...\")\n",
    "batches = list(create_batches(raw_text, batch_size=batch_size))\n",
    "print(f\"âœ… Created {len(batches)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"ðŸš€ Starting pretraining...\")\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "for iteration in range(num_iterations):\n",
    "    # Select batch (cycle through available batches)\n",
    "    batch_text = batches[iteration % len(batches)]\n",
    "    \n",
    "    # Training step\n",
    "    try:\n",
    "        loss = pretrain_step(model, batch_text, optimizer, device)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # Logging\n",
    "        if iteration % log_interval == 0:\n",
    "            print(f\"Iteration {iteration:3d}/{num_iterations} | Loss: {loss:.6f}\")\n",
    "            \n",
    "        # Check for NaN\n",
    "        if np.isnan(loss):\n",
    "            print(f\"âŒ NaN loss detected at iteration {iteration}\")\n",
    "            break\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error at iteration {iteration}: {str(e)}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nâœ… Training completed!\")\n",
    "print(f\"ðŸ“Š Total iterations: {len(losses)}\")\n",
    "print(f\"ðŸ“ˆ Final loss: {losses[-1]:.6f}\")\n",
    "print(f\"ðŸ“‰ Best loss: {min(losses):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss visualization\n",
    "print(\"ðŸ“Š Creating loss plot...\")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(losses, 'b-', linewidth=2, label='Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('PDCLMBase Pretraining Loss Curve')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Add loss statistics\n",
    "final_loss = losses[-1]\n",
    "min_loss = min(losses)\n",
    "plt.axhline(y=1.0, color='r', linestyle='--', alpha=0.7, label='Target Loss (1.0)')\n",
    "plt.text(0.02, 0.98, f'Final: {final_loss:.4f}\\nMin: {min_loss:.4f}', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "plot_path = \"pretrain_loss.png\"\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"ðŸ’¾ Loss plot saved: {plot_path}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation\n",
    "print(\"ðŸ” Validating trained model...\")\n",
    "\n",
    "model.eval()\n",
    "test_text = \"This is a test string for validation. \" * 50  # ~1500 chars\n",
    "\n",
    "with torch.no_grad():\n",
    "    val_loss = model(test_text)\n",
    "    print(f\"âœ… Validation loss: {val_loss.item():.6f}\")\n",
    "    \n",
    "    # Check if loss is in reasonable range\n",
    "    is_reasonable = 0 < val_loss.item() < 100\n",
    "    has_nan = torch.isnan(val_loss)\n",
    "    \n",
    "    print(f\"ðŸ“Š Validation results:\")\n",
    "    print(f\"  - Loss value: {val_loss.item():.6f}\")\n",
    "    print(f\"  - Has NaN: {has_nan.item()}\")\n",
    "    print(f\"  - Is reasonable (0-100): {is_reasonable}\")\n",
    "    print(f\"  - Target achieved (loss < 1.0): {val_loss.item() < 1.0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ TRAINING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_loss = losses[-1] if losses else float('inf')\n",
    "min_loss = min(losses) if losses else float('inf')\n",
    "success = final_loss < 1.0 and not np.isnan(final_loss)\n",
    "\n",
    "print(f\"ðŸ“Š Training Results:\")\n",
    "print(f\"  - Completed iterations: {len(losses)}/{num_iterations}\")\n",
    "print(f\"  - Final loss: {final_loss:.6f}\")\n",
    "print(f\"  - Best loss: {min_loss:.6f}\")\n",
    "print(f\"  - Loss reduction: {((losses[0] - final_loss) / losses[0] * 100):.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Goals:\")\n",
    "print(f\"  - Final loss < 1.0: {'âœ…' if final_loss < 1.0 else 'âŒ'}\")\n",
    "print(f\"  - No NaN values: {'âœ…' if not np.isnan(final_loss) else 'âŒ'}\")\n",
    "print(f\"  - Training completed: {'âœ…' if len(losses) == num_iterations else 'âŒ'}\")\n",
    "\n",
    "print(f\"\\nðŸ† Overall: {'SUCCESS' if success else 'NEEDS IMPROVEMENT'}\")\n",
    "\n",
    "if success:\n",
    "    print(\"\\nâœ… Faz-1 pretraining Ã§alÄ±ÅŸÄ±yor, loss dÃ¼ÅŸÃ¼yor!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Model optimization needed:\")\n",
    "    if np.isnan(final_loss):\n",
    "        print(\"  - Gradient clip ekle: torch.nn.utils.clip_grad_norm_\")\n",
    "    if final_loss >= 1.0:\n",
    "        print(\"  - Learning rate azalt veya batch size artÄ±r\")\n",
    "        print(\"  - Window size=512 sabit tut\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
