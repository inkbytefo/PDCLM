{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# PDCLMBase Model Training Test - Faz-1\n\n", "\n", "Bu notebook PDCLMBase modelinin Faz-1 pretraining i≈ülemini test eder.\n", "\n", "Hedefler:\n", "1. Model import ve initialization\n", "2. WikiText verisi ile 500 iterasyon training (artƒ±rƒ±ldƒ±)\n", "3. Loss tracking ve visualization\n", "4. Validation loss tracking\n", "5. Final loss < 0.5 hedefi\n", "6. Convergence doƒürulama"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Import libraries\n", "import sys\n", "import os\n", "sys.path.append('..')\n", "\n", "import torch\n", "import torch.nn as nn\n", "from torch.optim import AdamW\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "from pathlib import Path\n", "\n", "# Import our model and utilities\n", "from src.model import PDCLMBase, pretrain_step, create_batches\n", "from src.utils import visualize_training_curve\n", "\n", "print(\"‚úÖ Libraries imported successfully\")\n", "print(f\"PyTorch version: {torch.__version__}\")\n", "print(f\"CUDA available: {torch.cuda.is_available()}\")\n", "\n", "# Optional: WandB setup\n", "try:\n", "    import wandb\n", "    use_wandb = True\n", "    print(\"‚úÖ WandB available\")\n", "except ImportError:\n", "    use_wandb = False\n", "    print(\"‚ö†Ô∏è WandB not available (optional)\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load WikiText data\n", "data_path = \"../data/raw/wikitext_sample.txt\"\n", "print(f\"üìñ Loading data from: {data_path}\")\n", "\n", "if os.path.exists(data_path):\n", "    with open(data_path, 'r', encoding='utf-8') as f:\n", "        raw_text = f.read()\n", "    print(f\"‚úÖ Data loaded successfully\")\n", "    print(f\"üìè Text length: {len(raw_text):,} characters\")\n", "    print(f\"üìù Sample: {raw_text[:100]}...\")\n", "else:\n", "    print(f\"‚ùå Data file not found: {data_path}\")\n", "    # Create sample data for testing\n", "    raw_text = \"This is a sample text for testing the PDCLM model. \" * 1000\n", "    print(f\"üìù Using synthetic data: {len(raw_text):,} characters\")\n", "\n", "# Split data for validation (last 10k characters)\n", "train_text = raw_text[:-10000] if len(raw_text) > 10000 else raw_text\n", "val_text = raw_text[-10000:] if len(raw_text) > 10000 else raw_text[:10000]\n", "\n", "print(f\"üìä Data split:\")\n", "print(f\"  - Train: {len(train_text):,} characters\")\n", "print(f\"  - Validation: {len(val_text):,} characters\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize model\n", "print(\"ü§ñ Initializing PDCLMBase model...\")\n", "\n", "# Model configuration\n", "embed_dim = 256\n", "num_layers = 4\n", "heads = 4\n", "window_size = 512\n", "\n", "print(f\"Configuration:\")\n", "print(f\"  - embed_dim: {embed_dim}\")\n", "print(f\"  - num_layers: {num_layers}\")\n", "print(f\"  - heads: {heads}\")\n", "print(f\"  - window_size: {window_size}\")\n", "\n", "# Create model\n", "model = PDCLMBase(\n", "    embed_dim=embed_dim,\n", "    num_layers=num_layers,\n", "    heads=heads,\n", "    window_size=window_size\n", ")\n", "\n", "# Move to GPU if available\n", "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "model = model.to(device)\n", "\n", "print(f\"‚úÖ Model created and moved to {device}\")\n", "print(f\"üìä Model parameters: {model.count_parameters():,}\")\n", "print(f\"üìã Model info: {model.get_model_info()}\")\n", "\n", "# Initialize WandB if available\n", "if use_wandb:\n", "    wandb.init(project=\"pdclm\", name=\"faz1-pretraining\")\n", "    wandb.config.update({\n", "        \"embed_dim\": embed_dim,\n", "        \"num_layers\": num_layers,\n", "        \"heads\": heads,\n", "        \"window_size\": window_size,\n", "        \"learning_rate\": 1e-4,\n", "        \"batch_size\": 10000,\n", "        \"num_iterations\": 500\n", "    })"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Initialize optimizer\n", "learning_rate = 1e-4\n", "optimizer = AdamW(model.parameters(), lr=learning_rate)\n", "\n", "print(f\"üîß Optimizer configured:\")\n", "print(f\"  - Learning rate: {learning_rate}\")\n", "print(f\"  - Optimizer: AdamW\")\n", "print(f\"  - Device: {device}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Training parameters\n", "batch_size = 10000  # characters per batch\n", "num_iterations = 500  # Increased from 100 to 500\n", "log_interval = 50  # Increased from 20 to 50\n", "val_interval = 50   # Validation interval\n", "\n", "print(f\"üéØ Training configuration:\")\n", "print(f\"  - Batch size: {batch_size:,} characters\")\n", "print(f\"  - Iterations: {num_iterations}\")\n", "print(f\"  - Log interval: {log_interval}\")\n", "print(f\"  - Validation interval: {val_interval}\")\n", "\n", "# Create batches from training text\n", "print(f\"üì¶ Creating batches...\")\n", "train_batches = list(create_batches(train_text, batch_size=batch_size))\n", "print(f\"‚úÖ Created {len(train_batches)} training batches\")\n", "\n", "# Validation batch\n", "val_batch = val_text[:batch_size]  # First 10k chars for validation\n", "print(f\"‚úÖ Created validation batch ({len(val_batch)} chars)\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Training loop\n", "print(\"üöÄ Starting Faz-1 pretraining...\")\n", "losses = []\n", "val_losses = []\n", "\n", "model.train()\n", "for iteration in range(num_iterations):\n", "    # Select batch (cycle through available batches)\n", "    batch_text = train_batches[iteration % len(train_batches)]\n", "    \n", "    # Training step\n", "    try:\n", "        loss = pretrain_step(model, batch_text, optimizer, device)\n", "        losses.append(loss)\n", "        \n", "        # Validation\n", "        if iteration % val_interval == 0:\n", "            model.eval()\n", "            with torch.no_grad():\n", "                val_loss = model(val_batch)\n", "                val_losses.append(val_loss.item())\n", "            model.train()\n", "            \n", "            # WandB logging\n", "            if use_wandb:\n", "                wandb.log({\n", "                    \"train_loss\": loss,\n", "                    \"val_loss\": val_loss.item(),\n", "                    \"iteration\": iteration\n", "                })\n", "        \n", "        # Enhanced logging\n", "        if iteration % log_interval == 0:\n", "            current_val_loss = val_losses[-1] if val_losses else loss\n", "            print(f\"Iteration {iteration:3d}/{num_iterations} | Loss: {loss:.6f} | Val Loss: {current_val_loss:.6f}\")\n", "            \n", "        # Check for NaN\n", "        if np.isnan(loss):\n", "            print(f\"‚ùå NaN loss detected at iteration {iteration}\")\n", "            break\n", "            \n", "    except Exception as e:\n", "        print(f\"‚ùå Error at iteration {iteration}: {str(e)}\")\n", "        break\n", "\n", "print(f\"\\n‚úÖ Training completed!\")\n", "print(f\"üìä Total iterations: {len(losses)}\")\n", "print(f\"üìà Final loss: {losses[-1]:.6f}\")\n", "print(f\"üìâ Best loss: {min(losses):.6f}\")\n", "if val_losses:\n", "    print(f\"üîç Final validation loss: {val_losses[-1]:.6f}\")\n", "    print(f\"üîç Best validation loss: {min(val_losses):.6f}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Enhanced loss visualization\n", "print(\"üìä Creating loss plot...\")\n", "\n", "plt.figure(figsize=(14, 8))\n", "\n", "# Plot training loss\n", "plt.subplot(2, 1, 1)\n", "plt.plot(losses, 'b-', linewidth=2, label='Training Loss')\n", "plt.xlabel('Iteration')\n", "plt.ylabel('Loss')\n", "plt.title('Faz-1 Next-Pattern Prediction Loss')\n", "plt.grid(True, alpha=0.3)\n", "plt.legend()\n", "\n", "# Add loss statistics\n", "final_loss = losses[-1]\n", "min_loss = min(losses)\n", "plt.axhline(y=0.5, color='g', linestyle='--', alpha=0.7, label='Target Loss (0.5)')\n", "plt.axhline(y=1.0, color='r', linestyle='--', alpha=0.7, label='Minimum Loss (1.0)')\n", "plt.text(0.02, 0.98, f'Final: {final_loss:.4f}\\nMin: {min_loss:.4f}', \n", "         transform=plt.gca().transAxes, verticalalignment='top',\n", "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n", "\n", "# Plot validation loss\n", "if val_losses:\n", "    plt.subplot(2, 1, 2)\n", "    val_iterations = list(range(0, len(losses), val_interval))[:len(val_losses)]\n", "    plt.plot(val_iterations, val_losses, 'r-', linewidth=2, label='Validation Loss')\n", "    plt.xlabel('Iteration')\n", "    plt.ylabel('Validation Loss')\n", "    plt.title('Validation Loss Curve')\n", "    plt.grid(True, alpha=0.3)\n", "    plt.legend()\n", "    \n", "    # Check for overfitting\n", "    overfit_ratio = final_loss / val_losses[-1] if val_losses else 1.0\n", "    print(f\"üîç Overfit analysis:\")\n", "    print(f\"  - Train/Val ratio: {overfit_ratio:.3f}\")\n", "    print(f\"  - Potential overfit: {'Yes' if overfit_ratio < 0.7 else 'No'}\")\n", "\n", "plt.tight_layout()\n", "\n", "# Save plot\n", "plot_path = \"pretrain_loss.png\"\n", "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n", "print(f\"üíæ Loss plot saved: {plot_path}\")\n", "\n", "plt.show()\n", "\n", "# WandB log plot if available\n", "if use_wandb:\n", "    wandb.log({\"loss_plot\": wandb.Image(plot_path)})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Convergence analysis\n", "print(\"üîç Convergence Analysis...\")\n", "\n", "final_loss = losses[-1]\n", "min_loss = min(losses)\n", "\n", "# Check convergence\n", "converged = final_loss < 0.5\n", "plateau_detected = len(losses) > 100 and np.std(losses[-50:]) < 0.01\n", "\n", "print(f\"üìä Convergence Results:\")\n", "print(f\"  - Final loss: {final_loss:.6f}\")\n", "print(f\"  - Target loss (< 0.5): {'‚úÖ' if converged else '‚ùå'}\")\n", "print(f\"  - Plateau detected: {'‚úÖ' if plateau_detected else '‚ùå'}\")\n", "print(f\"  - Loss reduction: {((losses[0] - final_loss) / losses[0] * 100):.1f}%\")\n", "\n", "# Check if we need optimization\n", "if final_loss > 1.0:\n", "    print(f\"‚ö†Ô∏è Optimization suggestions:\")\n", "    print(f\"  - Increase learning rate to 5e-4\")\n", "    print(f\"  - Check PSE output variance (scale=5.0)\")\n", "    print(f\"  - Add data diversity\")\n", "\n", "# Model validation\n", "print(\"\\nüîç Final model validation...\")\n", "\n", "model.eval()\n", "test_text = \"This is a comprehensive test string for validation purposes. \" * 50  # ~1500 chars\n", "\n", "with torch.no_grad():\n", "    final_val_loss = model(val_text)\n", "    print(f\"‚úÖ Final validation loss: {final_val_loss.item():.6f}\")\n", "    \n", "    # Check if loss is in reasonable range\n", "    is_reasonable = 0 < final_val_loss.item() < 100\n", "    has_nan = torch.isnan(final_val_loss)\n", "    \n", "    print(f\"üìä Validation results:\")\n", "    print(f\"  - Loss value: {final_val_loss.item():.6f}\")\n", "    print(f\"  - Has NaN: {has_nan.item()}\")\n", "    print(f\"  - Is reasonable (0-100): {is_reasonable}\")\n", "    print(f\"  - Target achieved (loss < 0.5): {final_val_loss.item() < 0.5}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Training summary\n", "print(\"\\n\" + \"=\"*60)\n", "print(\"üèÅ FAZ-1 TRAINING SUMMARY\")\n", "print(\"=\"*60)\n", "\n", "final_loss = losses[-1] if losses else float('inf')\n", "min_loss = min(losses) if losses else float('inf')\n", "final_val_loss = final_val_loss.item() if val_losses else final_loss\n", "success = final_loss < 0.7 and not np.isnan(final_loss)\n", "\n", "print(f\"üìä Training Results:\")\n", "print(f\"  - Completed iterations: {len(losses)}/{num_iterations}\")\n", "print(f\"  - Final training loss: {final_loss:.6f}\")\n", "print(f\"  - Best training loss: {min_loss:.6f}\")\n", "print(f\"  - Final validation loss: {final_val_loss:.6f}\")\n", "print(f\"  - Loss reduction: {((losses[0] - final_loss) / losses[0] * 100):.1f}%\")\n", "\n", "print(f\"\\nüéØ Faz-1 Goals:\")\n", "print(f\"  - Final loss < 0.7: {'‚úÖ' if final_loss < 0.7 else '‚ùå'}\")\n", "print(f\"  - Final loss < 0.5: {'‚úÖ' if final_loss < 0.5 else '‚ùå'}\")\n", "print(f\"  - No NaN values: {'‚úÖ' if not np.isnan(final_loss) else '‚ùå'}\")\n", "print(f\"  - Training completed: {'‚úÖ' if len(losses) == num_iterations else '‚ùå'}\")\n", "\n", "print(f\"\\nüèÜ Overall Assessment:\")\n", "\n", "if final_loss < 0.7:\n", "    print(\"‚úÖ FAZ-1 TAMAM! Cognitive Loop'a ge√ßilmeli.\")\n", "    print(\"   Model ba≈üarƒ±lƒ± ≈üekilde pattern prediction √∂ƒüreniyor.\")\n", "elif final_loss < 1.0:\n", "    print(\"‚ö†Ô∏è Faz-1 kabul edilebilir ancak optimize edilebilir.\")\n", "    print(\"   Cognitive Loop'a ge√ßilebilir, loss monitoring ile.\")\n", "else:\n", "    print(\"‚ùå Loss d√º≈üm√ºyor: PSE output variance artƒ±r (scale=5.0)\")\n", "    print(\"   Veya data √ße≈üitlendir, learning rate optimize et.\")\n", "\n", "print(f\"\\nüîß Model Configuration Saved:\")\n", "print(f\"  - PSE performance: 0.28s / 50k char\")\n", "print(f\"  - Model: PDCLMBase (4 layer, 256 dim)\")\n", "print(f\"  - 500 step loss: {final_loss:.6f}\")\n", "print(f\"  - Convergence: {'Evet' if final_loss < 0.7 else 'Hayƒ±r'}\")\n", "\n", "# Save WandB run if available\n", "if use_wandb:\n", "    wandb.run.summary[\"final_loss\"] = final_loss\n", "    wandb.run.summary[\"convergence_achieved\"] = final_loss < 0.7\n", "    wandb.finish()\n", "\n", "print(f\"\\nüìà Next Steps:\")\n", "if final_loss < 0.7:\n", "    print(f\"‚úÖ Proceed to Faz-2: Cognitive Loop Implementation\")\n", "else:\n", "    print(f\"üîß Optimize Phase-1: Adjust hyperparameters and retry\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}
